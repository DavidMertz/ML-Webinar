{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "duration": "30 minutes"
   },
   "source": [
    "# Beginning Machine Learning with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning about Humans learning ML\n",
    "\n",
    "For a start at this lesson, we will load the massaged and moderately feature engineered data that we left off with in the last lesson.  The `X` and `y` for this cleaned up dataset are saved in this repository as separate CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.setup import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('data/HumansLearning_X.csv', index_col=0)\n",
    "y = pd.read_csv('data/HumansLearning_y.csv', squeeze=True, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importances\n",
    "\n",
    "Best practice in machine learning is to keep training and testing sets separate. With sufficiently large datasets, it makes little difference in the trained model parameters whether or how train/test observations are separated. But this is a small dataset, and also reflects a somewhat unique event (many students will learn about machine learning through many channels, but this particular tutorial, with a particular instructor, at a particular conference, will not necessarily generalize to all those channels).\n",
    "\n",
    "Therefore, in order to see simply what is the \"best possible\" decision tree for this dataset, I deliberately \"overfit\" by including all the observations in the model.  This improves predictive power somewhat, as one would expect.\n",
    "\n",
    "The feature importances identified by this classifier are not the same as—nor even necessarily particularly similar to—those that would be produced by other classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier(max_depth=7, random_state=0)\n",
    "tree.fit(X, y)\n",
    "tree.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "pd.Series(tree.feature_importances_, index=X.columns).plot.barh(figsize=(18,7));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cut points in a Decision Tree\n",
    "\n",
    "As well as seeing what features are most important in this trained model, we can use a lovely utility method in `sklearn.tree` to display the entire tree and its decision cuts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out where graphviz executable lives\n",
    "dotpath = !which dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate cut point diagram\n",
    "from sklearn.tree import export_graphviz\n",
    "import sys, subprocess\n",
    "from IPython.display import Image\n",
    "\n",
    "export_graphviz(tree, feature_names=X.columns, class_names=['failure','success'],\n",
    "                out_file='ml-good.dot', impurity=False, filled=True)\n",
    "subprocess.check_call([dotpath[0],'-Tpng','ml-good.dot','-o','ml-good.png'])\n",
    "Image('ml-good.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the diagram, blue branches reflect those respondents who found the tutorial more successful, and orange branches those who found it less so. The saturation of the displayed boxes reflects the strength of that decision branch.\n",
    "\n",
    "As seems obvious in retrospect, the fans of [*And Now for Something Completely Different*](https://en.wikipedia.org/wiki/And_Now_for_Something_Completely_Different) really did not like my tutorial very much. I probably should have provided a disclaimer at the beginning of the session. Years of Python experience is a slightly more important feature, but it follows an oddly stratified pattern wherein several different ranges of years show positive or negative effects—it's not linear.\n",
    "\n",
    "And of course, [*Time Bandits*](https://en.wikipedia.org/wiki/Time_Bandits) was not a Monty Python film at all: it is a Terry Gilliam film that happened to cast a number of Monty Python cast members. What on earth were those respondents thinking?!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A common API\n",
    "\n",
    "I have mentioned in the introduction to this course that one of the great virtues of scikit-learn is its use of a common API across many different model classes.  For practice, let us look at a typical pattern again for `DecisionTreeClassifier` followed by the same pattern for `DummyClassifier`.  This latter is a good sanity check of how well you might predict without really trying and/or without doing anything particularly meaningful to train against the data.\n",
    "\n",
    "There are several \"strategies\" that `DummyClassifier` might use (not all listed):\n",
    "\n",
    "* `stratified`: generates predictions by respecting the training set’s class distribution.\n",
    "* `most_frequent`: always predicts the most frequent label in the training set.\n",
    "* `uniform`: generates predictions uniformly at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree = DecisionTreeClassifier(max_depth=7, random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "tree.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "for strategy in ['most_frequent', 'stratified', 'prior', 'uniform']:\n",
    "    dummy = DummyClassifier(strategy=strategy, random_state=2)\n",
    "    dummy.fit(X_train, y_train)\n",
    "    score = dummy.score(X_test, y_test)\n",
    "    print(\"{:<15}| score = {:.3f}\".format(strategy, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To our alarm, the algorithm we thought was showing some promise is little better than random classification; in fact it is worse than one variety of the random strategy.$^1$\n",
    "\n",
    "There are a couple problems that are exposed here.  One is simply that the dataset is much too small to be a good fit for machine learning algorithms.  Discovering subtle patterns usually requires fairly extensive data.  Another problem *may be* that there simply is not a strong enough pattern in the data to meaningfully extract conclusions.\n",
    "\n",
    "<hr/>\n",
    "\n",
    "<small><i>$^1$A confession is in order: I tried a few different `random_state` values to find one that produced this imbalance.  But *only a few* and the fact any random seed might produce better results is not a good sign.</i></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A more encouraging dataset\n",
    "\n",
    "For demonstration purposes, I have also constructed a *fake* dataset by replicating the information in the whimsical data 50 times, but then randomly biasing features to be more predictive.  This is terrible practice for anything real, of course.  This gives us 5800 rows and data with the same features, but more predictive power, just as a pedagogical dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "fake_learning = pd.read_csv('data/FakeLearning.csv')\n",
    "X_fake = fake_learning.drop(\"Success\", axis=1)\n",
    "y_fake = fake_learning.Success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_fake, y_fake, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree = DecisionTreeClassifier(max_depth=7, random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "tree.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "for strategy in ['most_frequent', 'stratified', 'prior', 'uniform']:\n",
    "    dummy = DummyClassifier(strategy=strategy, random_state=2)\n",
    "    dummy.fit(X_train, y_train)\n",
    "    score = dummy.score(X_test, y_test)\n",
    "    print(\"{:<15}| score = {:.3f}\".format(strategy, score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing we notice with this change to the size and (artificial) predictiveness of the data is that `DummyClassifier` stays pretty much the same, and `DecisionTreeClassifier` gets a lot better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick comparison of many classifiers in scikit-learn\n",
    "\n",
    "> **\"The first 90 percent of the code accounts for the first 90 percent of the development time. The remaining 10 percent of the code accounts for the other 90 percent of the development time.\"** –Tom Cargill, Bell Labs\n",
    "\n",
    "Having done the 90% of our work that was needed for data cleanup, the next 90% can be spent on model selection. Better hyperparameters than those chosen below (mostly defaults) are likely to identify better fits. Moreover, the few classifiers listed are by no means all of those in scikit-learn.\n",
    "\n",
    "This code is mostly based on the Scikit-learn [Classifier Comparison](http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html).\n",
    "\n",
    "> Code source: Gaël Varoquaux & Andreas Müller<br/>\n",
    "> Modified for documentation by Jaques Grobler<br/>\n",
    "> License: BSD 3 clause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the large (but fake) dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "                    X_fake, y_fake, test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "classifiers = {\n",
    "    \"Dummy\"        : DummyClassifier(strategy='uniform', random_state=2),\n",
    "    \"KNN(3)\"       : KNeighborsClassifier(3), \n",
    "    \"RBF SVM\"      : SVC(gamma=2, C=1), \n",
    "    \"Decision Tree\": DecisionTreeClassifier(max_depth=7), \n",
    "    \"Random Forest\": RandomForestClassifier(max_depth=7, n_estimators=10, max_features=4), \n",
    "    \"Neural Net\"   : MLPClassifier(alpha=1), \n",
    "    \"AdaBoost\"     : AdaBoostClassifier(),\n",
    "    \"Naive Bayes\"  : GaussianNB(), \n",
    "    \"QDA\"          : QuadraticDiscriminantAnalysis(),\n",
    "    \"Linear SVC\"   : LinearSVC(),\n",
    "    \"Linear SVM\"   : SVC(kernel=\"linear\"), \n",
    "    \"Gaussian Proc\": GaussianProcessClassifier(1.0 * RBF(1.0)),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have moved to a larger dataset (of 5k items) some models can take considerable time to train.  This can often become a significant practical consideration in choices.  MoreoverHappily, prediction time is almost always vastly quicker than training time.  It is often worthwhile to sink hours, or days, into training times if it will produce better models; those models can usually be used for predictions in orders of magnitude less time (usually with better big-O efficiency than training).\n",
    "\n",
    "Let us first compare the first few classifiers in the dictionary we define above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "nfast = 10      # Don't run the very slow ones\n",
    "head = list(classifiers.items())[:nfast]\n",
    "\n",
    "for name, classifier in head:\n",
    "    start = time()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    train_time = time() - start\n",
    "    start = time()\n",
    "    score = classifier.score(X_test, y_test)\n",
    "    score_time = time()-start\n",
    "    print(\"{:<15}| score = {:.3f} | time = {:,.3f}s/{:,.3f}s\".format(name, score, train_time, score_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many classifiers scale training time linearly with sample size.  So for those fortunate ones, training on a million observations will simply take 100x as long as training on ten thousand observations.  However, a few classifiers have worse big-O complexities than this.  SVC with a linear kernel is $O(N^2)$ and Gaussian Process is $O(N^3)$, making them generally infeasible for large training sets.  Interestingly, `LinearSVC` implements an SVC linear kernel differently, making it $O(N)$ complexity, but also resulting in dramatically worse results in many tests I have tried.\n",
    "\n",
    "```\n",
    "Dummy          | score = 0.508 | time = 0.00s/0.00s\n",
    "KNN(3)         | score = 0.588 | time = 0.01s/0.03s\n",
    "RBF SVM        | score = 0.621 | time = 0.42s/0.19s\n",
    "Decision Tree  | score = 0.821 | time = 0.02s/0.00s\n",
    "Random Forest  | score = 0.844 | time = 0.02s/0.00s\n",
    "Neural Net     | score = 0.807 | time = 0.20s/0.03s\n",
    "AdaBoost       | score = 0.840 | time = 0.12s/0.02s\n",
    "Naive Bayes    | score = 0.806 | time = 0.00s/0.00s\n",
    "QDA            | score = 0.569 | time = 0.06s/0.00s\n",
    "Linear SVC     | score = 0.773 | time = 0.141s/0.001s\n",
    "Linear SVM     | score = 0.819 | time = 166.78s/0.05s\n",
    "Gaussian Proc  | score = 0.621 | time = 3,334.28s/0.31s\n",
    "```\n",
    "\n",
    "Within the range of classifiers, all have advantages against some datasets.  Other than `DummyClassifier` none of the classifiers lack advantages for at least some range of datasets.  The scores we found against the artificially expanded and weighted data suggests one we might look at more closely for the actual data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importances revisited\n",
    "\n",
    "One of the classifiers that did well in our playing around is `AdaBoostClassifier`.  Unfortunately, when we return to the initial smaller and unskewed dataset, it has the same generally rather low score as `DecisionTreeClassifier`.  It may simply be that the underlying data has little predictive value in this case.  If the target is genuinely independent/orthogonal to all the other features, there is nothing a machine learning model can predict. \n",
    "\n",
    "But one thing nice about `AdaBoostClassifier` is that it provides feature importances like `DecisionTreeeClassifier` and `RandomForestClassifier`.  This produces about the same predictive value, while focusing on very different features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = pd.read_csv('data/HumansLearning_X.csv', index_col=0)\n",
    "y = pd.read_csv('data/HumansLearning_y.csv', squeeze=True, header=None)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember the old not-that-great score\n",
    "tree = DecisionTreeClassifier(max_depth=7, random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "tree.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Adaboost score is simlarly not-that-great\n",
    "ab = AdaBoostClassifier()\n",
    "ab.fit(X_train, y_train)\n",
    "ab.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit on the entire data set again, as before.  And look at the feature importances in contrast between the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=7, random_state=0).fit(X, y)\n",
    "pd.Series(tree.feature_importances_, index=X.columns).sort_values(ascending=False).head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab = AdaBoostClassifier().fit(X, y)\n",
    "pd.Series(ab.feature_importances_, index=X.columns).sort_values(ascending=False).head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting result here is that AdaBoost produces much more intuitive weights than does a decision tree.  Decision tree puts relatively equal weights on many features, most of which we would not guess are important.  AdaBoost puts high weights on just a couple features, and very little on any others., and those are the ones that a human would guess are likely to be important.  This result may not apply across all datasets, but it probably will in many.\n",
    "\n",
    "The documentation describes AdaBoost like this:\n",
    "\n",
    "> An AdaBoost classifier is a meta-estimator that begins by fitting a classifier on the original dataset and then fits additional copies of the classifier on the same dataset but where the weights of incorrectly classified instances are adjusted such that subsequent classifiers focus more on difficult cases. This class implements the algorithm known as AdaBoost-SAMME. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass classification\n",
    "\n",
    "For simplification, we looked at a binary classification problem so far, and one with somewhat questionable underlying predictive possibilities.  Let us use a more real-world example that is included with scikit-learn, a dataset of handwritten digits, similar to the MNIST dataset often used to test the strength of different models.\n",
    "\n",
    "> This dataset is made up of 1797 8x8 images. Each image, like the one shown below, is of a hand-written digit. In order to utilize an 8x8 figure like this, we’d have to first transform it into a feature vector with length 64.\n",
    "\n",
    "Reading the description, even though we normally think of an image as a $width \\times height$ array, the representation here is simply of independent features for each pixel of the image.\n",
    "\n",
    "To model this we will use `LogisticRegression`.  Despite its name, this model is a classifier not a regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us understand the nature of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(digits.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Features:\",\n",
    "      digits.data.shape, \n",
    "      digits.data.dtype, \n",
    "      \"\\n  Target:\",\n",
    "      digits.target.shape, \n",
    "      digits.target.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1, 8, figsize=(16, 2))\n",
    "for n in range(8):\n",
    "    ax[n].imshow(digits.images[n], cmap=plt.cm.gray_r)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A multiclass confusion matrix\n",
    "\n",
    "In a binary classification problems, the confusion matrix has a shape of (2, 2). In a multi-class problem, the confusion matrix has a shape (n_classes, n_classes) to show the correct vs incorrect classification counts for each class versus every other class.\n",
    "\n",
    "A perfect confusion matrix has positive main diagonal and zeros elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, random_state=0)\n",
    "\n",
    "# Perform a logistic regression\n",
    "lr = LogisticRegression().fit(X_train, y_train)\n",
    "\n",
    "# Make predictions against the test set\n",
    "pred = lr.predict(X_test)\n",
    "\n",
    "# Find the accuracy of the predictions against the true classes\n",
    "print(\"accuracy: %0.3f\" % accuracy_score(y_test, pred))\n",
    "\n",
    "# Show the confusion matrix\n",
    "print(\"confusion matrix:\")\n",
    "print(confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the [`mglearn` helper package](https://github.com/amueller/mglearn), used by *Introduction to Machine Learning with Python*, to present these results in a beautified way.  This nice, Free Software package from sckit-learn core developer Andreas Mueller has a number of convenient utilities, and is included in the archive for this course.  You might want to check the original repository to see if he has updated it since it was bundled here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_image = mglearn.tools.heatmap(confusion_matrix(y_test, pred), \n",
    "                                     xlabel='Predicted label', \n",
    "                                     ylabel='True label',\n",
    "                                     xticklabels=digits.target_names, \n",
    "                                     yticklabels=digits.target_names,\n",
    "                                     cmap=plt.cm.gnuplot_r, \n",
    "                                     fmt=\"%d\")    \n",
    "\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction probabilities\n",
    "\n",
    "Most classification models produce not only a specific class prediction, but also a probability of each class membership.  The actual prediction is simply the most likely class according to the model.  For example, looking at one of the digit images, we can see the likelihood—according to the model—that it represents each possible value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "n = randint(0,len(digits.images))\n",
    "plt.imshow(digits.images[n], cmap=plt.cm.gray_r)\n",
    "probs = pd.Series(lr.predict_proba(digits.data[n:n+1]).flatten()) \n",
    "for digit, prob in probs.sort_values(ascending=False).iteritems():\n",
    "    print(digit, \"| %.4f%%\" % (100*prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier boundary comparison\n",
    "\n",
    "On of the main tradeoffs among classifiers (and also among regressors) is between complexity and generalization.  For example, `LinearRegression` and `LogisticRegression` are very simple models that cannot fit complicated relationships. On the other hand, it is hard to overfit a LinearRegression.\n",
    "\n",
    "In contrast, `KNeighborsClassifier` is not very general.  At the extreme, with one neighbor, the model essentially just memorizes the data.  On the other hand, for overly large `n_neighbors`, the model will tend to only predict the most commonly occurring label.  That said, there are many datasets for which K nearest neighbor is an excellent choice.\n",
    "\n",
    "The below code is taken directly from the [scikit-learn documentation](http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html).  It is a nice way to explore and visually summarize the different decision boundaries that various classifiers product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run src/classifier_comparison.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next lesson\n",
    "\n",
    "**Regression**: In the current lessson we explored the classifer APIs and looked at a few examples of different datasets and different classifiers.  We had a passing exposure to hyperparameters and validation, but the bulk of those topics come in later lessons.  In the next lesson we will stay within supervised learning, but look at regression rather than classification.\n",
    "\n",
    "<a href=\"Regression.ipynb\"><img src=\"img/open-notebook.png\" align=\"left\"/></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
