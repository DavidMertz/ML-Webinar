{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "duration": "30 minutes"
   },
   "source": [
    "# Beginning Machine Learning with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting our Hands Dirty\n",
    "\n",
    "In this lesson, we go through several techniques within scikit-learn, many of which we return to explore in more detail in subsequent lessons.  Having a sense of the overall steps and results one sees in a machine learning task provides a good reference to more in-depth exploration later.\n",
    "\n",
    "Whenever we perform supervised learning, our workflow will resemble the diagram here.  That is, we need to divide our data into training and testing sets, and within that, many \"columns\" of data are known as *features* and just one is known as the *target*.  The difference between classification and regression is simply whether the target is categorical or continuous.  Some similar models exist for both types of target, other are specific to one or the other.\n",
    "\n",
    "<img src='img/supervised_workflow.png' width=40% align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machines Learning about Humans Learning about Machine Learning\n",
    "\n",
    "I gave the first tutorial at AnacondaCON 2018, on machine learning with scikit-learn. I spoke there to about 120 attendees.\n",
    "\n",
    "The attendees of my session were an excellent group of learners and experts. But I decided I wanted to know even more about these people than I could find by looking at their faces and responding to their questions. So I asked them to complete a slightly whimsical form at the end of the 3 hour  tutorial. Just who are these people, and what can scikit-learn tell us about which of them benefitted most from the tutorial?\n",
    "\n",
    "In the interest open data science, the collection of answers given by attendees is available under a [CC-BY-SA 4.0 license](https://creativecommons.org/licenses/by-sa/4.0/legalcode), and is part of the [GitHub repository for this course](https://github.com/DavidMertz/ML-Live-Beginner). The anonymized data is [available as a CSV file](https://github.com/DavidMertz/ML-Live-Beginner/blob/master/data/Learning%20about%20Humans%20learning%20ML.csv). \n",
    "\n",
    "The attendees of this course are well described as:\n",
    "\n",
    "> **\"Humans learning about machines learning about humans learning about machine learning.\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be great to collect a larger dataset for future revisions of this analysis.  Please complete this [Machine Learning with scikit-learn survey](https://goo.gl/pghpzD).  Updated results will appear on the [GitHub repository](https://github.com/DavidMertz/ML-Live-Beginner) from time to time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Whimsical Dataset\n",
    "\n",
    "Data never arrives at the workstation of a data scientist quite clean, no matter how much validation is attempted in the collection process. The respondent data is no exception. Using the familiar facilities in Pandas, we can improve the initial data before applying scikit-learn to it. In particular, I failed to validate the field \"`Years of post-secondary education (e.g. BA=4; Ph.D.=10)`\" as a required integer. Also, the \"`Timestamps`\" added by the form interface are gratuitous for these purposes—they are all within a couple minutes of each other, but the order or spacing is unlikely to have any value to our models.\n",
    "\n",
    "Let's start to look at the data (here and elsewhere a few libraries and configurations we want are imported from a module in the archive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.setup import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data\n",
    "fname = join('data', \"Learning about Humans learning ML.csv\")\n",
    "humans = pd.read_csv(fname)\n",
    "\n",
    "# Drop unused column\n",
    "humans.drop('Timestamp', axis=1, inplace=True)\n",
    "\n",
    "# Add an improved column\n",
    "humans['Education'] = (humans[\n",
    "    'Years of post-secondary education (e.g. BA=4; Ph.D.=10)']\n",
    "                       .str.replace(r'.*=','')\n",
    "                       .astype(int))\n",
    "\n",
    "# Then drop the one it is based on\n",
    "humans.drop('Years of post-secondary education (e.g. BA=4; Ph.D.=10)', \n",
    "            axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eyeballing Data\n",
    "\n",
    "At the start of your work on a dataset, it is always useful to take a look at it to get a \"feel\" for the data. For this example, the dataset is small enough that it wouldn't be absurd to look at every single data point in it.  However, many of the datasets you will work with will have hundreds of thousands or millions of rows, and item by item examination is impossible.  For these cases, we need to look at representative values and aggregations of features.\n",
    "\n",
    "If the dataset can be read as a Pandas DataFrame, overview inspection is particularly easy and friendly.\n",
    "\n",
    "> **\"80% of the time spent doing data analysis is doing data cleanup.\"** –Every Data Scientist\n",
    "\n",
    "I am currently writting a book for Addison Wesley, tentatively titled _Data Cleaning for Data Science: Doing the Other 80% of the work_.  Keep your eyes open for the title, probably available late 2020.  That is a different topic than this webinar, but it is something every data scientist should think about quite a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\".join(humans.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "humans.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, let us give these shorter names to all the columns (and ones that are Python identifiers that we can use for attribute-style access.  There is nothing functional in this change, but it often makes later code look nicer.  \n",
    "\n",
    "Looking at a few rows of data often can help correct or improve our understanding of the meaning, range, units, common values, etc. of the data we wish to construct models around. In a great many cases, common sense can prevent chasing down dead ends that take hours or days of needless time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "humans.columns = ['Fav_lang', 'Fav_movie', 'Experience', 'Sklearn', \n",
    "                  'Age', 'Humans_Machines', 'Fav_Game', 'Success', 'Education']\n",
    "humans.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the metadata and a basic statistical aggregation of the data is generally useful also.  Pandas DataFrames provide a very easy way to look at this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "humans.describe(include=['int', 'int64', 'float', 'object'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleanup\n",
    "\n",
    "It would be useful to explore aspects of the (simple) data offline to get practice. In the summary view a few data quality issues jump out. This is universal to real world datasets. \n",
    "\n",
    "I am doubtful that two 3 year-olds were in my audience. More likely, a couple 30-somethings mistyped entering their ages. A 99 year-old is possible, but that also seems more likely to be a placeholder value used by some respondent. While the description of what is meant by the integer \"Education\" was probably underspecified, it still feels like the -10 years of education is more likely to be a data entry problem than an intended indicator.\n",
    "\n",
    "However, *the data we have is the data we must analyze*.  For this lesson, we will not actually cleanup this data further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "humans[humans.Age == 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot Encoding\n",
    "\n",
    "Several features of the data represent a small number of discrete categories.  For many or most algorithms, using one-hot enconding of categorical data is more effective than using raw categories or converting to integers. Basically, all those columns that have a small number of unique values—and specifically values that are not ordinal, even implicitly—are are categorical.\n",
    "\n",
    "One-hot encoding makes less difference for the decision tree and random forest classifiers used in this lesson than it might for other classifiers and regressors, but it rarely hurts. We perform the encoding with `pandas.get_dummies()`, but you could equally use `sklearn.preprocessing.LabelBinarizer` to accomplish the same goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_dummies = pd.get_dummies(humans)\n",
    "list(human_dummies.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification: Choosing Features and a Target\n",
    "\n",
    "Let us use scikit-learn to model the respondents. In particular, we would like to know whether other features of attendees are a good predictor of how successful they found the tutorial. A very common pattern you will see in machine learning based on starting DataFrames is to drop one column for the X features, and keep that one for the y target.\n",
    "\n",
    "In my analysis, I felt a binary measure of success was more relevant than a scalar measure initially collected as a 1-10 scale. Moreover, if the target is simplified this way, it becomes appropriate to use a *classification* algorithm as opposed to a *regression* algorithm. It would be a mistake to treat the 1-10 scale as a categorical consisting of 10 independent labels—there is something inherently ordinal about these labels, although scikit-learn will happily calculate models as if there is not. On the other hand, responses to this ordinal question is generally non-uniform in distribution, usually with a clustering of values near the top values.\n",
    "\n",
    "This is a place where subject matter judgement is needed by a data scientist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = human_dummies.drop(\"Success\", axis=1)\n",
    "y = human_dummies.Success >= 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We selected a cutoff for success scores  >=8 will approximately evenly divide the data into \"Yes\" and \"No\" categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conventional Names and Shapes\n",
    "\n",
    "In almost all machine learning discussions, you will see the names capital-X and lowercase-y for the feature set and the target. The idea here is that the capital stands for the independent variables, but in general one expects there to be multiple such feature variables. The target consists of just one dependent variable, and hence its lowercase. The feature set and the target will always have the same number of rows.\n",
    "\n",
    "In some types of models—especially with some neural networks, but not with most scikit-learn models—we will predict multiple target features at once.  In that case we use a capital-Y to indicate the target array.\n",
    "\n",
    "Using X and y to distinguish independent and dependent variables is widespread in many areas of mathematics. Moreover, you will often see the features within X named $x_1$, $x_2$, $x_3$, and so on in more academic texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split\n",
    "\n",
    "While using [sklearn.model_selection.StratifiedKFold](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html) is a more rigorous way of evaluating a model, for quick-and-dirty experimentation, using `train_test_split()` is usually the easiest approach. In either case, the basic principle is that you want to avoid overfitting by training on different data than you use to test your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "print(\"Training features/target:\", X_train.shape, y_train.shape)\n",
    "print(\"Testing features/target:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a later lesson we return to more details about train/test splits.  For now, this creates relative independence of training data from the test set used for evaluation.  A deeper issue remains about whether the analyzed sample is truly representative of *all* the uncollected data of this type in the rest of the world.\n",
    "\n",
    "In some sense, overfitting is a non-issue for this dataset if we think of it as *complete*—i.e. every response from a one-time event that can never be exactly repeated.  But in that strict sense of the particularity of the data, machine learning is irrelevant since we *have* every possible measurement.\n",
    "\n",
    "We can visualize the several breakdowns of our individual data items:\n",
    "\n",
    "<img src='img/train_test_split_matrix.png' width=\"66%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing an Algorithm: Decision Trees and Random Forests\n",
    "\n",
    "An interesting thing happened in trying a few models out. While `RandomForestClassifier` is incredibly powerful, and very often produces the most accurate predictions among all classifiers, for this particular data a single `DecisionTreeClassifer` does better. Viewers might want to think about why this turns out to be true and/or experiment with hyperparameters to find a more definite explanation; other classifiers might perform better still also, of course.\n",
    "\n",
    "I will note that choosing the best max_depth for decision tree family algorithms is largely a matter of trial and error. You can search the space in a nice high level API using [sklearn.model_selection.GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html), but it often suffices to use a basic Python loop like:\n",
    "\n",
    "```python\n",
    "for n in range(1,20):\n",
    "    tree = DecisionTreeClassifier(max_depth=n)\n",
    "    tree.fit(X_train, y_train)\n",
    "    print(n, tree.score(X_test, y_test))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=10, random_state=1)\n",
    "rf.fit(X_train, y_train)\n",
    "rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree = DecisionTreeClassifier(max_depth=17, random_state=1)\n",
    "tree.fit(X_train, y_train)\n",
    "tree.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice element of random forests and decision trees—but not of all types of classifiers—is that they provide probabilities of their prediction of each class, not only a single answer.  We can see that soem answers are ones the model is highly confident about, while others it only gives a slight preference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Lesson\n",
    "\n",
    "**Classification**: In the current lessson we cleaned up our dataset enough to being to fit and score a classification model.  In the next lesson we will look more deeply at our initial classifier, and beging to compare it to a variety of other classifiers available in scikit-learn.\n",
    "\n",
    "<a href=\"Classification.ipynb\"><img src=\"img/open-notebook.png\" align=\"left\"/></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
