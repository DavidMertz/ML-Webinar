{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beginning Machine Learning with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring hyperparameters\n",
    "\n",
    "In the introductory lesson and at various points in other earlier lessons, we utilized **hyperparameters** to models to tune their performance.  In this lesson we will look at hyperparamters a bit more systematically, and especially look at *grid search* which is a nice API to use in exploring hyperparametric space.\n",
    "\n",
    "While there is overlap in the hyperparameters used by different models, the same name often has a somewhat different meaning because the underlying mathematical process is different.  Moreover, different models usually have mostly different collections of hyperparameters that pertain to them.  Learning the available hyperparameters is a matter of learning about the individual model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from src.setup import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Wisconsin breast cancer dataset\n",
    "\n",
    "For this lesson, we will look at another sample dataset included with scikit-learn.  The cancer dataset has 30 features and a binary target of \"malignant\" or \"benign.\"  This dataset is moderate sized with 569 samples.  \n",
    "\n",
    "Our goal in this lesson is not to identify the *optimal* classifier and hyperparameters, but simply to explore how to work with the parametric space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer = load_breast_cancer()\n",
    "cancer.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cancer.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive classification\n",
    "\n",
    "For now, we use K Nearest Neighbors (KNN) classification, mostly because it is easy to understand.  The general idea of KNN is simply to identify the K points that are \"closest\" to a test point or newly observed point, and let the plurality win.$^1$  KNN does quite well for numerous classification and regression problems.\n",
    "\n",
    "<hr/>\n",
    "<small>$^1$<i>The winner may not be a majority.  For example with 8 nearest neighbors and four classes, we might have a predicted point whose closest neighbors are 2 points from each class.  The tie is broken arbitrarily by the order of the training data.  Even if we had 9 neighbors and the count of those nearby were <code>{A:3, B:2, C:2, D:2}</code>, letting A win would still be with only ⅓ of neighbors \"voting\" for A.</i></small> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    cancer.data, cancer.target, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A 93% score might strike you as quite good, especially having seen much worse ones in other examples.  However, the thing we are trying to predict is literally a life or death matter, which makes the number seem less impressive.  Moreover, we have not here teased out the differences between false positives and false negatives in that accuracy score.  Presumably, in this domain we would rather have more false positives than false negatives because unnecessarily treatment (or unnecessary additional testing) is less bad than a missed diagnosis.\n",
    "\n",
    "In a later lesson we look at metrics in more detail. For this lesson, we will only look at this model `.score()` method as our optimization goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring one hyperparameter\n",
    "\n",
    "The most obvious hyperparameter for KNN classification is the number of neighbors used.  Many aspects of the data—from number of samples, to number of dimensions, to multi-modality in univariate distribution of features—can greatly affect the \"right\" answer.  Moreover, if we really want to arrive at the best classification, we should look at scaling issues that will be glossed over here but discussed in a later lesson on feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for k in range(1, 40):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    score = knn.score(X_test, y_test)\n",
    "    scores.append(score)\n",
    "    \n",
    "scores = pd.Series(scores, index=range(1,40), name=\"Score\")\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is easier to see a pattern if we visualize the trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(range(1, 40), scores,marker='o', markerfacecolor='red', markersize=5)\n",
    "plt.xlabel('n_neighbors')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Model response to number of neighbors');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring many hyperparameters\n",
    "\n",
    "While `n_neighbors` is the most obvious hyperparameter, others can significantly affect the accuracy (or other metrics) as well.  All scikit-learn models have default values for their hyperparameters, but how good those choices are is very domain specific.\n",
    "\n",
    "Two additional hyperparameters of interest to KNN are `weight` and `metric`.  There are a few other hyperparameters, but those are used for performance considerations not for fundamental behavior of trained models.\n",
    "\n",
    "By default, `weight` is `\"uniform\"` meaning that it simply gives one \"vote\" to each of the closest neighbors.  But `\"distance\"` is quite plausible; it weights each such neighbor by the inverse of distance (with cutoffs for only K neighbors considered nonetheless).\n",
    "\n",
    "By default, `metric` is `\"minkowski\"` which is a generalization of Pythagorean distance to higher dimensions.  But `\"manhattan\"` distance is also often useful; it measure the \"city blocks\" to get from point to point (i.e. the sum of the distance in each direction).  Other are available and occasionally better choices.\n",
    "\n",
    "| identifier | distance function\n",
    "|------------|----------------------\n",
    "| euclidean  | $$ \\sqrt{\\vphantom{\\int}}{\\sum (x-y)^2} $$ \n",
    "| manhattan  | $$ \\sum{|x-y|} $$\n",
    "| chebyshev  | $$ \\max {\\Big\\{x-y\\Big\\}} $$\n",
    "| minkowski  | $$ \\sum{\\big(|x-y|^p\\big)^{1/p}} $$\n",
    "| wminkowski | $$ \\Big({\\sum |w \\cdot (x-y)|^p}\\Big)^{1/p} $$\n",
    "| seuclidean | $$ \\sqrt{\\vphantom{\\int}}{\\sum \\frac{(x-y)^2}{V}} $$\n",
    "| mahalanobis| $$ \\sqrt{(x-y)^{'} \\cdot V^{-1} \\cdot (x-y)} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try combining a couple of these hyperparameters in the same search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "metrics = ['minkowski', 'manhattan', 'euclidean', 'chebyshev']\n",
    "K = range(1, 18, 2)\n",
    "scores = np.empty((len(metrics),len(K)))\n",
    "\n",
    "for x, k in enumerate(K):\n",
    "    for y, metric in enumerate(metrics):\n",
    "        knn = KNeighborsClassifier(n_neighbors=k, metric=metric)\n",
    "        knn.fit(X_train, y_train)\n",
    "        score = knn.score(X_test, y_test)\n",
    "        scores[y, x] = score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap(scores, \n",
    "        title='Model response to 2 hyperparameters',\n",
    "        xticklabels=list(K),\n",
    "        yticklabels=metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearchCV\n",
    "\n",
    "So far, so good.  It would not *too* hard to keep track of the best model discovered within the inner loop.  And any Python programmer could construct more nested loops to search over 3, or 4, or 5, different hyperparameters.  \n",
    "\n",
    "We could store all the scores in a parameter grid of N dimensions.  \n",
    "\n",
    "Maybe while we are at it, it would be nice to remember the training and scoring times different hyperparameters take. \n",
    "\n",
    "It could be useful to allow for different scoring metrics to be performed within the nested search of hyperparameters.  Or actually perform multiple different scoring functions that may inform the quality of hyperparameter sets differently.\n",
    "\n",
    "We might also want our code to perform more robust and configurable train/test split strategies.\n",
    "\n",
    "But really, it is much easier to take the `GridSearchCV` function from scikit-learn that does all of this for us and is well-tested to avoid any pitfalls, bugs, or edge cases we might overlook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'n_neighbors': range(1, 18, 2),\n",
    "              'weights': ['uniform', 'distance'],\n",
    "              'metric': ['minkowski', 'manhattan', 'euclidean', 'chebyshev']\n",
    "             }\n",
    "\n",
    "grid = GridSearchCV(KNeighborsClassifier(), parameters)\n",
    "# Best fit over cross-product of parameter space, cross-validated\n",
    "model = grid.fit(cancer.data, cancer.target)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying the best hyperparameters\n",
    "\n",
    "One additional nice detail of `GridSearchCV` is that by default once a best set of hyperparameters is identified, the model is refit against the entire dataset rather than only the training split.  This can improve accuracy while avoiding overfitting in the initial hyperparameter choice.\n",
    "\n",
    "The object delivered in an attribute—but also usable directly as the grid search model object itself—reflects this improved refitting (if the argument `refit` is kept as the default `True` value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.best_params_,'\\n')\n",
    "print(model.best_estimator_,'\\n')\n",
    "print(model.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(cancer.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the search space\n",
    "\n",
    "Some the information collected about the search is times taken for steps.  Given a search across a large, multi-dimensional, hyperparameter space can require many combinations, the fitting can take a considerable time. `KNeighborsClassifier` was chosen for this lesson in part because it is a very fast model.\n",
    "\n",
    "Moreover, while KNN performs pretty much equally quickly across a range of hyperparameters, that is definitely **not true** of many other models.  In some cases, a hyperparameter choses among different algorithms with very different performance characteristics.  In others, a hyperparameter chooses among threshhold type values that can greatly affect convergence rates or other computational details.  Being able to know not only that this combination of hyperparameters has better *accuracy*, but also what the relative *performance* of each is, can be imporant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(pd.DataFrame(grid.cv_results_)\n",
    "   .set_index('rank_test_score')\n",
    "   .sort_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next lesson\n",
    "\n",
    "In the next several lessons, we turned first to Clustering, and a discussion of *unsupervised learning* techniques.  The few lessons past that will look broadly at *feature engineering*, which includes decomposition, an unsupervised technique of a different sort.\n",
    "\n",
    "<a href=\"Clustering.ipynb\"><img src=\"img/open-notebook.png\" align=\"left\"/></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
