{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with scikit-learn\n",
    "\n",
    "Sometimes the features you have available in your initial data have little predictive strength when used in the most straightforward way.  This might be true almost regardless of choice of model class and hyperparameters.  And yet it might also be true that there are synthetic features latent in the data that are highly predictive, but that have to be *engineered* (mechanically, rather than sample-wise modification) to produce powerful features.\n",
    "\n",
    "At the same time, a highly dimension model—whether of high dimension because of the initial data collection or because of creation of extra synthetic features—may lend itself less well to modeling techniques.  In these cases, it can be more computationally tractable, as well as more predictive, to work with a subset of all available features.\n",
    "\n",
    "We will spend several lessons that can be thought of broadly as \"Feature Engineering.\"  This first lesson focuses on *decompositions*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from src.setup import *\n",
    "from src.whiten import data, show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction\n",
    "\n",
    "There are two general approaches to *reducing* the number of dimensions (i.e. features) in a dataset.  One is by creating synthetic features that globally combine the raw features.  Several different mathematical techniques are available for doing this.\n",
    "\n",
    "The other means by which we might reduce dimensions is simply by discarding ones that seem to have little significance to the model.  Very often models actually perform *better* by removing features that are either purely independent of the target or are largely redundant with other features (i.e. highly correlated).  In every case, models can be trained *faster* with fewer dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decomposition\n",
    "\n",
    "PCA is the oldest and most widely used method for decomposition of dimensional information.  Other methods of decomposition are also provided by scikit-learn.  In broad concept they do something similar, but each shows strengths relative to different datasets; understanding the difference is a mixture of domain familiarity and trial-and-error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miscellaneous decompositions\n",
    "\n",
    "This lesson will not specifically discuss all of the decomposition classes available in `sklearn.decomposition`.  A complete list with brief descriptions from the [documentation](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.decomposition) is below.  The discussion and examples of reconstructed faces in the [user guide](http://scikit-learn.org/stable/modules/decomposition.html) is very useful.\n",
    "\n",
    "| Class name          | Description\n",
    "|---------------------|------------------------------------\n",
    "| DictionaryLearning  | Dictionary learning\n",
    "| FactorAnalysis      | Factor Analysis (FA)\n",
    "| FastICA             | FastICA: a fast algorithm for Independent Component Analysis.\n",
    "| IncrementalPCA      | Incremental principal components analysis (IPCA).\n",
    "| KernelPCA           | Kernel Principal component analysis (KPCA)\n",
    "| LatentDirichletAllocation | Latent Dirichlet Allocation with online variational Bayes algorithm\n",
    "| MiniBatchDictionaryLearning | Mini-batch dictionary learning\n",
    "| MiniBatchSparsePCA  | Mini-batch Sparse Principal Components Analysis\n",
    "| NMF                 | Non-Negative Matrix Factorization (NMF)\n",
    "| PCA                 | Principal component analysis (PCA)\n",
    "| SparsePCA           | Sparse Principal Components Analysis (SparsePCA)\n",
    "| SparseCoder         | Sparse coding\n",
    "| TruncatedSVD        | Dimensionality reduction using truncated SVD (aka LSA).\n",
    "| dict_learning       | Solves a dictionary learning matrix factorization problem.\n",
    "| dict_learning_online| Solves a dictionary learning matrix factorization problem online.\n",
    "| fastica             | Perform Fast Independent Component Analysis.\n",
    "| sparse_encode       | Sparse coding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal component analysis (PCA)\n",
    "\n",
    "Principal component analysis (PCA) is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components. The transformation is defined in such a way that the first principal component accounts for as much of the variability in the data as possible, and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An explanation written by [Hernán Eche for Stack Exchange](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues) contained a very nice animation illustrating minimization of variance to find a first principal component. It is easier to understand with a 2-D parametric space, as in the animation.\n",
    "\n",
    "![PCA animation](img/PCA-animation.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at sample data. We will choose the Wisconsin breast cancer dataset we worked with in a previous lesson.  Recall that it has 30 features measuring a variety of numeric medical diagnostic results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First thing, we load the dataset and apply standard scaling to the features.  Later in this lesson we will talk more about scaling, and we have brushed on it in a few previous lessons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(cancer.data)\n",
    "X_scaled = scaler.transform(cancer.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a standard train/test split as we have with almost all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, cancer.target, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For pending comparison, let us remind ourselves of how the raw data preforms using two classifers: `LinearRegression` and `KNeighborsClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "(LinearRegression()\n",
    "    .fit(X_train, y_train)\n",
    "    .score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "(KNeighborsClassifier()\n",
    "    .fit(X_train, y_train)\n",
    "    .score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we transform the parametric space of the features into just two dimensions that contain the maximum amount of information that **can be** represented in two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X_scaled)\n",
    "X_pca = pca.transform(X_scaled)\n",
    "print(\"Original shape: %s\" % str(X_scaled.shape))\n",
    "print(\"Reduced shape: %s\" % str(X_pca.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at a scatter plot, we can see that just two dimensions already get rather good differentiation visually.  Notice that these two components each represent an arbitrary combination of all the actual observational measurements in the dataset.  Therefore, they do not have any obvious English description other than \"first component\" and \"second component.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 1st vs 2nd principal component, color by class\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], \n",
    "            c=cancer.target, s=10)\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.xlabel(\"First principal component\")\n",
    "plt.ylabel(\"Second principal component\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With just two components retained, the linear regression performs a bit worse, but KNN (that is much stronger so far for this data, in any case), performs basically equally well as with all 30 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_pca, cancer.target, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(LinearRegression()\n",
    "    .fit(X_train, y_train)\n",
    "    .score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(KNeighborsClassifier()\n",
    "    .fit(X_train, y_train)\n",
    "    .score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how the two PCA components are derived by linear combination of the original 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or more visually (but only some original features shown for legibility):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not all original features shown...\n",
    "nfeat = 12\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.matshow(pca.components_[:,:nfeat], cmap='viridis')\n",
    "plt.yticks([0, 1], [\"first component\", \"second component\"])\n",
    "plt.colorbar()\n",
    "plt.xticks(range(nfeat),\n",
    "           cancer.feature_names[:nfeat], rotation=90, ha='left');\n",
    "plt.xlabel(\"pca_components_cancer\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotation, sphering and whitening\n",
    "\n",
    "When we perform a decomposition on, we always emphasize the \"most important synthetic features.\"  The result of this for PCA specifically is that, by definition, the variance decreases with each PCA feature.  *Whitening* and *sphering* are synonyms for balancing these synthetic features.\n",
    "\n",
    "There can be a secondary de-emphasis of some features that is too strong.  It depends on the specific kind of model used, but for many models a numeric feature ranging from 0 to 100 will simply have more effect than a feature varying from 0 to 1 just because it contributes *bigger* numbers to the calculation.  Usually it is better to let a model select the importances of features than to judge it in advance with feature engineering.\n",
    "\n",
    "The discussion in a later lesson on scaling addresses this point in more detail, but it is a common enough concern in PCA transforms that the class builds in an argument to do it automatically.  This often saves us the need to rescale data a second time after the transform, and is generally a cleaner approach, where appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only two initial features for illustration, \n",
    "# but in general we would have a highly dimensionality\n",
    "show(data, \"Parametric space for two features\", \n",
    "     \"Raw Feature 1\", \"Raw Feature 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clearly see that the most prominent pattern in these data points happens along the diagonal of the two measures.  Or said another way, the features are highly correlated (but not completely).  Extracting the synthetic component is simply a *rotation* within the parametric space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(PCA(whiten=False).fit_transform(data), \n",
    "     \"PCA Features\", \"Synthetic Feature 1\", \"Synthetic Feature 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To feed these engineered features into a model downstream, we likely want to utilize synthetic features with a balanced scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show(PCA(whiten=True).fit_transform(data), \n",
    "     \"PCA Features (whitened)\", \"Synthetic Feature 1\", \"Synthetic Feature 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-negative matrix factorization (NMF)\n",
    "\n",
    "NMF finds two non-negative matrices whose product approximates the non-negative matrix X. This factorization can be used, for example, for dimensionality reduction, source separation or topic extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "nmf = NMF(n_components=2)\n",
    "nmf.fit(cancer.data)\n",
    "X_nmf = nmf.transform(cancer.data)\n",
    "print(\"Original shape: %s\" % str(cancer.data.shape))\n",
    "print(\"Reduced shape: %s\" % str(X_pca.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 1st vs 2nd principal component, color by class\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(X_nmf[:, 0], X_nmf[:, 1], \n",
    "            c=cancer.target, s=10)\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.xlabel(\"First NMF component\")\n",
    "plt.ylabel(\"Second NMF component\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_nmf, cancer.target, random_state=1)\n",
    "print(LinearRegression()\n",
    "    .fit(X_train, y_train)\n",
    "    .score(X_test, y_test))\n",
    "print((KNeighborsClassifier()\n",
    "    .fit(X_train, y_train)\n",
    "    .score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "Latent Dirichlet Allocation is a generative probabilistic model for collections of discrete dataset such as text corpora. It is also a topic model that is used for discovering abstract topics from a collection of documents.\n",
    "\n",
    "This Bayesian technique has proven very useful for identifing hidden unobserved features in textual data sources.  It is not particularly well suited to the kind of numeric data we see in the cancer dataset, but we use that simply to show API usage here. Basically, it is the same as other decomposition classes—and generally the same as all the models throughout scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=2)\n",
    "lda.fit(cancer.data)\n",
    "X_lda = lda.transform(cancer.data)\n",
    "print(\"Original shape: %s\" % str(cancer.data.shape))\n",
    "print(\"Reduced shape: %s\" % str(X_lda.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 1st vs 2nd principal component, color by class\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(X_lda[:, 0], X_lda[:, 1], \n",
    "            c=cancer.target, s=10)\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.xlabel(\"First LDA component\")\n",
    "plt.ylabel(\"Second LDA component\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_lda, cancer.target, random_state=1)\n",
    "print(LinearRegression()\n",
    "    .fit(X_train, y_train)\n",
    "    .score(X_test, y_test))\n",
    "print((KNeighborsClassifier()\n",
    "    .fit(X_train, y_train)\n",
    "    .score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Independent component analysis (ICA)\n",
    "\n",
    "Independent component analysis separates a multivariate signal into additive subcomponents that are maximally independent. It is implemented in scikit-learn using the Fast ICA algorithm. Typically, ICA is not used for reducing dimensionality but for separating superimposed signals. \n",
    "\n",
    "> In PCA the basis you want to find is the one that best explains the variability of your data. The first vector of the PCA basis is the one that best explains the variability of your data (the principal direction) the second vector is the 2nd best explanation and must be orthogonal to the first one.\n",
    "\n",
    "> In ICA the basis you want to find is the one in which each vector is an independent component of your data, you can think of your data as a mix of signals and then the ICA basis will have a vector for each independent signal.\n",
    "\n",
    "> In a more practical way we can say that PCA helps when you want to find a reduced-rank representation of your data and ICA helps when you want to find a representation of your data as independent sub-elements. In layman terms PCA helps to compress data and ICA helps to separate data.\n",
    "\n",
    "Quote credit: [Luis Argerich](https://www.quora.com/What-is-the-difference-between-PCA-and-ICA), Data Science Professor at the University of Buenos Aires (UBA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import FastICA\n",
    "ica = FastICA(n_components=2)\n",
    "ica.fit(cancer.data)\n",
    "X_ica = ica.transform(cancer.data)\n",
    "print(\"Original shape: %s\" % str(cancer.data.shape))\n",
    "print(\"Reduced shape: %s\" % str(X_ica.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 1st vs 2nd principal component, color by class\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(X_ica[:, 0], X_ica[:, 1], \n",
    "            c=cancer.target, s=10)\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.xlabel(\"First ICA component\")\n",
    "plt.ylabel(\"Second ICA component\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One might hope that since ICA is often used for separation of \"signals\" in noisy data, the cancer data might do better than PCA at detecting the \"benign\" and \"malignant\" signals that might be reflected in the indirect medical measurements.\n",
    "\n",
    "At least without other parameterization, or other models, and looking at exactly two components, ICA does moderately worse than PCA.  Only very slightly so though; it is possible that other feature engineering might allow ICA to emerge as a good technique for this problem area. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_ica, cancer.target, random_state=1)\n",
    "print(LinearRegression()\n",
    "    .fit(X_train, y_train)\n",
    "    .score(X_test, y_test))\n",
    "print((KNeighborsClassifier()\n",
    "    .fit(X_train, y_train)\n",
    "    .score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-distributed Stochastic Neighbor Embedding.\n",
    "\n",
    "t-SNE is a tool to visualize high-dimensional data. It converts similarities between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data. t-SNE has a cost function that is not convex, i.e. with different initializations we can get different results.\n",
    "\n",
    "That is to say, t-SNE can often help models in similar ways to what PCA, NMF, LDA, or ICA do.  But it also is especially useful to create low-dimensional representations (i.e. two dimensions can actually fit on your computer monitors or printed books)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example that shows t-SNE well is the digit dataset that we saw in an earlier lesson.  Let us load that data, and decompose it with both PCA and t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "print(digits.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(10, 5),\n",
    "                         subplot_kw={'xticks':(), 'yticks': ()})\n",
    "for ax, img in zip(axes.ravel(), digits.images):\n",
    "    ax.imshow(img, cmap=plt.get_cmap('Greys'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a PCA model\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(digits.data)\n",
    "\n",
    "# transform the digits data onto the first two principal components\n",
    "digits_pca = pca.transform(digits.data)\n",
    "colors = [\"#476A2A\", \"#7851B8\", \"#BD3430\", \"#4A2D4E\", \"#875525\",\n",
    "          \"#A83683\", \"#4E655E\", \"#853541\", \"#3A3120\",\"#535D8E\"]\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.xlim(digits_pca[:, 0].min(), digits_pca[:, 0].max())\n",
    "plt.ylim(digits_pca[:, 1].min(), digits_pca[:, 1].max())\n",
    "for i in range(len(digits.data)):\n",
    "    # actually plot the digits as text instead of using scatter\n",
    "    plt.text(digits_pca[i, 0], digits_pca[i, 1], str(digits.target[i]),\n",
    "             color = colors[digits.target[i]],\n",
    "             fontdict={'weight': 'bold', 'size': 9})\n",
    "plt.xlabel(\"first principal component\")\n",
    "plt.ylabel(\"second principal component\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(random_state=42)\n",
    "# use fit_transform instead of fit, as TSNE has no transform method:\n",
    "digits_tsne = tsne.fit_transform(digits.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.xlim(digits_tsne[:, 0].min(), digits_tsne[:, 0].max() + 1)\n",
    "plt.ylim(digits_tsne[:, 1].min(), digits_tsne[:, 1].max() + 1)\n",
    "for i in range(len(digits.data)):\n",
    "    # actually plot the digits as text instead of using scatter\n",
    "    plt.text(digits_tsne[i, 0], digits_tsne[i, 1], str(digits.target[i]),\n",
    "             color = colors[digits.target[i]],\n",
    "             fontdict={'weight': 'bold', 'size': 9})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next lesson\n",
    "\n",
    "**Feature Expansion**: In this lesson we (mostly) dealt with information preserving transformations.  While the end use of decompositions very often involves discarding some low-information synthetic features, in principal a transformation like PCA allows us to keep all N original dimensions, just using a different orthonormal bases.  Most of the other decompositions are similar in that we *could* retain N dimensions to match the original N features, but decide usually to select a smaller number.\n",
    "\n",
    "In the next sub-lesson we look at ways of producing new features *per se* rather than simply new bases in parametric space.  These ideas are clearly related ones, but the nex sub-lesson shifts focus modestly.\n",
    "\n",
    "<a href=\"FeatureExpansion.ipynb\"><img src=\"img/open-notebook.png\" align=\"left\"/></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
